---
title: "IU Fingerprinting Vignette"
author: "Lily Koffman"
date: "2023-02-28"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(cache = TRUE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
library(tidyverse) 
library(purrr)
library(broom)
library(ggplot2)
library(gt)
library(viridis)
library(gridExtra)
options(dplyr.summarise.inform = FALSE)
`%notin%` <- Negate(`%in%`)
theme_set(theme_minimal())
get_density <- function(x, y, ...) {
      dens <- MASS::kde2d(x, y, ...)
      ix <- findInterval(x, dens$x)
      iy <- findInterval(y, dens$y)
      ii <- cbind(ix, iy)
      return(dens$z[ii])
    }
```

```{r read in data}
df_all <- read_csv("df_all.csv")[,-1]
all_data <- read_csv("all_data.csv")[,-1]
```

```{r functions}
map_dfr_progress <- function(.x, .f, ..., .id = NULL) {
  .f <- purrr::as_mapper(.f, ...)
  pb <- progress::progress_bar$new(total = length(.x), force = TRUE)
  
  f <- function(...) {
    pb$tick()
    .f(...)
  }
  purrr::map_dfr(.x, f, ..., .id = .id)
}

get_grid_data  <- function(subject, time_lags, gcell_size, location, data){
  # function to get grid cell data from one subject 
  # select ID, signal, time (seconds), filter to subject 
  df <- data %>% dplyr::select(ID2, paste("signal_", location, sep = ""), J) %>% 
    rename(signal = paste("signal_", location, sep = ""),
           ID = ID2) %>% filter(ID==subject)
  # max_signal <- round(max(df$signal), 0) 
  max_signal <- 3  # we set max signal to 3 based on EDA, but could take actual max signal 
  n_seconds <- max(df$J) # number of total seconds for the subject 
  seconds <- rep(seq(1, n_seconds, 1), each=length(time_lags)) # vector of seconds and lags so that we can iterate over both
  lags <- rep(time_lags, n_seconds) # vector of lags 
  # function to get the grid data for one second, one lag 
  get_grid_data_lagsec <- function(second, lag){
    # filter to one second 
    df %>% filter(J==second) %>% dplyr::select(signal) %>% mutate(
      lag_signal = lag(signal, n = lag)) %>%   # for each second, calculate signal and lagged signal 
      mutate(cut_sig = cut(signal, breaks = seq(0, max_signal, by = gcell_size), include.lowest = T),
                 cut_lagsig = cut(lag_signal, breaks = seq(0, max_signal, by = gcell_size), include.lowest = T)) %>% 
      drop_na() %>% # count # points in each "grid cell" 
      count(cut_sig, cut_lagsig, .drop=FALSE) %>% mutate(lag_hz = lag, ID = subject, J = second, cell = paste(cut_sig, cut_lagsig, lag_hz)) %>%
      dplyr::select(n, ID, J, cell)
  }
  # apply above function over all seconds and lags, return df
  map2_dfr(.x = seconds, .y = lags, 
           .f = get_grid_data_lagsec) %>% pivot_wider(id_cols = c(ID, J), names_from = cell, values_from = n) 
  # apply across all seconds and lags 
}


fit_model <- function(subject, n_predictors, train, test, threshold){
  # first filter to grids above threshold 
  grids <- train %>% filter(ID==subject) %>% pivot_longer(cols = -ID) %>% 
    mutate(lag = sub("^.+] ", "", name)) %>% group_by(name, lag) %>% 
    summarize(value = sum(value)) %>% group_by(lag) %>% mutate(
      group_sum = sum(value)
    ) %>% ungroup() %>% mutate(
      pct = value/group_sum
    ) %>% filter(pct >= threshold) %>% dplyr::select(name)  %>% unlist() %>% unname()
  # make outcome binary 
  train$class <- ifelse(train$ID==subject, 1, 0)
  test$class <- ifelse(test$ID==subject, 1, 0)
  # with the training data, fit a logistic regression with each predictor individually and get p-value 
  # select __ lowest p values based on n_predictors input 
  # if we want to use all predictors, then we set n_predictors=Inf
  if(!is.infinite(n_predictors)){
    imp_grids <- train %>% dplyr::select(all_of(grids)) %>%
      map(~glm(train$class ~.x, data = train, family = binomial(link="logit"))) %>%
      map(summary.glm) %>% map(c("coefficients")) %>% map_dbl(8) %>% as.data.frame() %>% rename(pval = ".") %>% slice_min(pval, n=n_predictors) %>% rownames()
  }
  else{
    imp_grids <- grids
  }
  # select important grids from train and test data 
  tmp <- train %>% dplyr::select(c(class, all_of(imp_grids))) 
  tmp_test <- test %>% dplyr::select(all_of(imp_grids))
  # fit logistic regression 
  mod <- glm(class ~ ., data = tmp, family=binomial(link="logit"))
  # extract predictors
  pred <- predict.glm(mod, newdata = tmp_test, type = "response")
  # return data frame with predictions, model, and subject 
  return(cbind(pred, rep(subject, length(pred)), test$ID) %>% data.frame() %>% rename(model = V2, true_subject = V3))
}

extract_models <- function(subject, n_predictors, train, test, threshold){
  # first filter to grids above threshold 
  grids <- train %>% filter(ID==subject) %>% pivot_longer(cols = -ID) %>% 
    mutate(lag = sub("^.+] ", "", name)) %>% group_by(name, lag) %>% 
    summarize(value = sum(value)) %>% group_by(lag) %>% mutate(
      group_sum = sum(value)
    ) %>% ungroup() %>% mutate(
      pct = value/group_sum
    ) %>% filter(pct >= threshold) %>% dplyr::select(name)  %>% unlist() %>% unname()
  # make outcome binary 
  train$class <- ifelse(train$ID==subject, 1, 0)
  test$class <- ifelse(test$ID==subject, 1, 0)
  # with the training data, fit a logistic regression with each predictor individually and get p-value 
  # select __ lowest p values based on n_predictors input 
  # if we want to use all predictors, then we set n_predictors=Inf
  if(!is.infinite(n_predictors)){
    imp_grids <- train %>% dplyr::select(all_of(grids)) %>%
      map(~glm(train$class ~.x, data = train, family = binomial(link="logit"))) %>%
      map(summary.glm) %>% map(c("coefficients")) %>% map_dbl(8) %>% as.data.frame() %>% rename(pval = ".") %>% slice_min(pval, n=n_predictors) %>% rownames()
  }
  else{
    imp_grids <- grids
  }
  # select important grids from train and test data 
  tmp <- train %>% dplyr::select(c(class, all_of(imp_grids))) 
  tmp_test <- test %>% dplyr::select(all_of(imp_grids))
  # fit logistic regression 
  mod <- glm(class ~ ., data = tmp, family=binomial(link="logit"))
  # extract predictors
  pred <- predict.glm(mod, newdata = tmp_test, type = "response")
  # return data frame with predictions, model, and subject 
  return(mod)
}

summarize_preds <- function(all_predictions){
  # group by the true subject and the model
  # for example, predictions from model k, row j is prob(row of j data is from subject k)
  # get the mean predicted probabilities and see if highest predicted prob is "correct" 
  df <- all_predictions %>% group_by(true_subject, model) %>% summarize(mean_pred = mean(pred)) %>% 
    group_by(true_subject) %>% summarize(
      maxprob = max(mean_pred),
      prediction = model[mean_pred==maxprob], # which subject has highest predicted probability 
      probsubj = mean_pred[true_subject==model] # which probability comes from the "true" model 
    ) %>% mutate(
      correct = ifelse(as.numeric(prediction)==true_subject, 1, 0)
    ) 
  df %>% filter(correct==0) %>% dplyr::select(-correct) %>% gt() %>% 
    tab_header(title = "Incorrect Subjects Summary",
               subtitle = paste("Correct prediction rate: ", round(sum(df$correct)/nrow(df), 2))) %>%
    cols_label(true_subject = "Subject",
               maxprob = "Highest Predicted Probability",
               prediction = "Predicted Subject",
               probsubj = "True Subject Predicted Probability") %>% fmt_number(columns = c(2,4), decimals=2)
}
# function to plot results 
plot_preds <- function(all_predictions, title=NULL){
  subj <- unique(all_predictions$true_subject)
  vlines <- subj
  col1 <-"firebrick3"; col2 <- "springgreen3"
  df <- all_predictions %>% group_by(true_subject, model) %>% summarize(mean_pred = mean(pred))  %>% group_by(true_subject) %>% summarize(
    maxprob = max(mean_pred),
    prediction = model[mean_pred==maxprob],
    probsubj = mean_pred[true_subject==model]
  ) %>% mutate(
    correct = ifelse(as.numeric(prediction)==true_subject, 1, 0)
  ) 
  num_wrong <- nrow(df[df$correct!=1,])
  g <-  all_predictions %>% group_by(true_subject, model) %>% summarize(mean_pred = mean(pred)) %>% mutate(
    correct = ifelse(true_subject==model, "Correct", "Incorrect"))%>% ggplot(aes(x = true_subject, y= mean_pred, col = as.factor(correct)))+theme_minimal()+
    labs(x = "Subject ID", y= "Avg. Predicted Probability", subtitle = paste(num_wrong, "Subjects Incorrect"))+ 
    scale_color_manual(name = "", values=c("Correct"=col2, "Incorrect" = col1))+
    geom_vline(xintercept=vlines, col = "lightgrey", alpha=.2)+scale_x_continuous(breaks=subj)+
    geom_jitter(size=3, alpha=.8, width = .1)+theme(legend.position="bottom")+scale_y_continuous(limits=c(0,1))
  if(is.null(title)){g}
  else{g + ggtitle(paste(title))}
}

# function to run all of the above in one line 
pipeline <- function(data, location, training_pct, time_lags, gcell_size, threshold, n_predictors){
  # get vector of subjects 
  subjects <- data %>% dplyr::select(ID2) %>% distinct() %>% unlist()
  # get grid data (predictors)  
  data_all <- map_dfr(subjects, get_grid_data,
                      time_lags = time_lags, gcell_size = gcell_size, location=location, data=data)
  # get training and testing data based on pct supplied 
  data_train <- data_all %>% group_by(ID) %>% 
    mutate( maxJ = max(J)) %>% filter(J <= maxJ*training_pct) %>% dplyr::select(-c(J, maxJ)) %>% ungroup()
  data_test <- data_all %>% group_by(ID) %>% 
    mutate( maxJ = max(J)) %>% filter(J > maxJ*training_pct) %>%  dplyr::select(-c(J, maxJ)) %>% ungroup()
  # fit models 
  all_predictions <- map_dfr(.x = subjects, n_predictors=n_predictors, train = data_train,
                             test = data_test,threshold = threshold,
                             .f = fit_model)
  # print summary 
  summarize_preds(all_predictions = all_predictions)
  # store plot 
  g <- plot_preds(all_predictions = all_predictions)
  # save predictions 
  return(list(predictions = all_predictions, plot = g, data = data_all))
}

get_roc <- function(all_predictions, individual = F){
  test_preds_wide <- all_predictions %>% group_by(model) %>% dplyr::select(-true_subject) %>% group_split() %>% bind_cols() %>% dplyr::select(-starts_with("model"))
  colnames(test_preds_wide) <- unique(all_predictions$true_subject) # rename columns to match models 
  true <- all_predictions$true_subject[1:nrow(test_preds_wide)] # get true subjects 
  multiroc <- pROC::multiclass.roc(true, as.matrix(test_preds_wide))
  if(individual == F){
    return(round(multiroc$auc, 2))
    print(round(multiroc$auc, 2))
  }
  else{
    split_df <- all_predictions %>%  mutate(class =ifelse(model==true_subject, 1, 0)) %>% group_by(model) %>% 
      group_split()
    roc_objects <- map(.x = split_df, response = class, predictor = pred, .f = pROC::roc)
    g.list <- pROC::ggroc(roc_objects)
    g <- g.list+scale_color_viridis_d(name = "Subject")+labs(x = "Specificity", y = "Sensitivity", title = "AUC Curves",
                                                             subtitle = paste("Multiple AUC:", round(multiroc$auc, 2)))+theme_minimal() 
    aucs <- map_dbl(.x = roc_objects, .f = pROC::auc) 
    print(g)
    return(list(multiroc = multiroc$auc, aucs = aucs, plot = g))
  }
}



```

### Data Summary

For each subject, gait acceleration series measured simultaneously at the right wrist, left hip, right ankle, and left ankle. The data are collected at 100 Hz.

```{r data summary}
tab1 <- df_all %>% group_by(ID2) %>% summarize(
 seconds = max(J),
 mins = round(seconds/60, 2)) %>% slice(1:16) %>% gt() %>% tab_header(title= "Summary of Times by Session and Recording") %>% 
  cols_label(ID2 = "Subject", seconds = "Number Seconds", mins = "Minutes") 
tab2 <- df_all %>% group_by(ID2) %>% summarize(
 seconds = max(J),
 mins = round(seconds/60, 2)) %>% slice(17:32) %>% gt() %>% tab_header(title= "Summary of Times by Session and Recording") %>% 
  cols_label(ID2 = "Subject", seconds = "Number Seconds", mins = "Minutes") 
gtExtras::gt_two_column_layout(list(tab1, tab2), vwidth=250)


```

\vspace{.3 in}

We see that we have between approximately 9 and 13 minutes of data for each individual.

### Data Visualization

We look at the raw times series for a subject, and also the lag 15 second densities.

```{r data vis}
# we want to visualize a few subjects

df_all %>% filter(ID2==1&J<=5) %>% dplyr::select(ID2, time_s_2, signal_lw, signal_lh, signal_ra, signal_la) %>% pivot_longer(cols = 3:6) %>%
  ggplot(aes(x = time_s_2, y = value, col = name))+geom_line()+labs(
  x = "Time (sec)", y = "Acceleration (g)", title = "5 Seconds of Data from Various Locations",
  subtitle= "Subject 1"
)+scale_x_continuous(breaks=seq(0, 5,1))+scale_colour_viridis_d(name = "Location", labels = c("L. Ankle", "L. Hip", "L. Wrist", "R. Ankle"))

df_all %>% filter(ID2==1&J<=5) %>% dplyr::select(ID2, time_s_2, signal_lw, signal_lh, signal_ra, signal_la) %>% pivot_longer(cols = 3:6) %>%
  ggplot(aes(x = time_s_2/100, y = value, col = name))+geom_line()+labs(
  x = "Time (sec)", y = "Acceleration (m/s^2)", title = "5 Seconds of Data from Various Locations",
  subtitle= "Subject 1"
)+scale_x_continuous(breaks=seq(0, 5,1))+facet_wrap(.~name)+scale_color_viridis_d()+theme(legend.position="none")


df_all %>% filter((ID2==1|ID2==2)&J<=5) %>% dplyr::select(ID2, time_s_2, signal_lw) %>%
  ggplot(aes(x = time_s_2/100, y = signal_lw, col = factor(ID2)))+geom_line()+labs(
  x = "Time (sec)", y = "Acceleration (m/s^2)", title = "5 Seconds of Data", subtitle = "Left Wrist")+scale_x_continuous(breaks=seq(0, 5,1))+scale_color_viridis_d(name = "Subject")


```

```{r plot densities }
dens <- df_all %>% filter(ID2==1) %>% dplyr::select(signal_lw, time_s_2) %>% mutate(
      lag_signal = lag(signal_lw, n = 15)  # for each second, calculate signal and lagged signal 
    ) %>% filter(!is.na(lag_signal))
dens$density <- get_density(dens$signal_lw, dens$lag_signal, n = 100)

lw <- ggplot(dens, aes(x = signal_lw, y  = lag_signal, color = density))+geom_point()+scale_color_viridis(name = "Density")+
  labs( x = "Signal (m/s2)", y = "Lag Signal (m/s2)", title = "Densities Subject 1, Left Wrist, Lag = 0.15s")

dens <- df_all %>% filter(ID2==1) %>% dplyr::select(signal_la, time_s_2) %>% mutate(
      lag_signal = lag(signal_la, n = 15)  # for each second, calculate signal and lagged signal 
    ) %>% filter(!is.na(lag_signal))
dens$density <- get_density(dens$signal_la, dens$lag_signal, n = 100)

la <- ggplot(dens, aes(x = signal_la, y  = lag_signal, color = density))+geom_point()+scale_color_viridis(name = "Density")+
  labs( x = "Signal (m/s2)", y = "Lag Signal (m/s2)", title = "Densities Subject 1, Left Ankle, Lag = 0.15s")
grid.arrange(lw, la)

# subject 2
dens <- df_all %>% filter(ID2==2) %>% dplyr::select(signal_lw, time_s_2) %>% mutate(
      lag_signal = lag(signal_lw, n = 15)  # for each second, calculate signal and lagged signal 
    ) %>% filter(!is.na(lag_signal))
dens$density <- get_density(dens$signal_lw, dens$lag_signal, n = 100)

lw <- ggplot(dens, aes(x = signal_lw, y  = lag_signal, color = density))+geom_point()+scale_color_viridis(name = "Density")+
  labs( x = "Signal (m/s2)", y = "Lag Signal (m/s2)", title = "Densities Subject 2, Left Wrist, Lag = 0.15s")

dens <- df_all %>% filter(ID2==2) %>% dplyr::select(signal_la, time_s_2) %>% mutate(
      lag_signal = lag(signal_la, n = 15)  # for each second, calculate signal and lagged signal 
    ) %>% filter(!is.na(lag_signal))
dens$density <- get_density(dens$signal_la, dens$lag_signal, n = 100)

la <- ggplot(dens, aes(x = signal_la, y  = lag_signal, color = density))+geom_point()+scale_color_viridis(name = "Density")+
  labs( x = "Signal (m/s2)", y = "Lag Signal (m/s2)", title = "Densities Subject 2, Left Ankle, Lag = 0.15s")
grid.arrange(lw, la)
```

### Model Fitting

We calculate the grid cell predictors using time lags of 0.15, 0.30, 0.45, 0.60, 0.75, and 0.90 seconds and grid cell size of 0.25 m/s2. We split the data into 50% training and 50% testing. We filter the training data to the grid cells with at least 0.1% of the data; we use these grid cells as predictors in a logistic regression. We use one vs. rest logistic regression to obtain predicted probabilities for each subject on the testing data. Then we calculate the average probability for each subject and determine whether the model has identified the correct individual. Finally, we plot the predicted probabilities, calculate the overall AUC, and plot each individual AUC curve.

```{r model fitting}
training_pct <- 0.50 
threshold <- 0.001
n_predictors=Inf

data_train <- all_data %>% group_by(ID)  %>% filter(J <= 200) %>% dplyr::select(-J) %>% ungroup()
data_test <- all_data %>% group_by(ID) %>% filter(J > 200 & J<=400) %>%  dplyr::select(-J) %>% ungroup()
subjects <- unique(all_data$ID)
all_predictions <- map_dfr(.x = subjects, n_predictors=n_predictors, train = data_train,
                           test = data_test,threshold = threshold,
                           .f = fit_model)
```

### Left Wrist Results

```{r}
summarize_preds(all_predictions)
plot_preds(all_predictions)
roc <- get_roc(all_predictions, individual=T)

```

```{r, eval=F}
data_train <- all_data %>% group_by(ID) %>% 
  filter(J <= 200) %>% dplyr::select(-J) %>% ungroup()
data_test <- all_data %>% group_by(ID) %>% 
  filter(J > 200 & J <=380) %>%  dplyr::select(-J) %>% ungroup()
subjects <- unique(all_data$ID)
all_predictions_orig <- map_dfr(.x = subjects, n_predictors=n_predictors, train = data_train,
                           test = data_test,threshold = threshold,
                           .f = fit_model)
```

```{r, eval=F}
summarize_preds(all_predictions_orig)
plot_preds(all_predictions_orig)
get_roc(all_predictions_orig, individual=T)
```

### Distance Metrics

```{r, eval=FALSE}
training_pct <- 0.75 

data_train <- all_data %>% group_by(ID) %>% 
  mutate( maxJ = max(J)) %>% filter(J <= maxJ*training_pct) %>% dplyr::select(-c(J, maxJ)) %>% ungroup()
data_test <- all_data %>% group_by(ID) %>% 
  mutate( maxJ = max(J)) %>% filter(J > maxJ*training_pct) %>%  dplyr::select(-c(J, maxJ)) %>% ungroup()
```

We're interested in looking at a \`\`distance" between two subjects' data. We consider the Hamming distance, which is the number of bit positions in which the two bits are different between two vectors. For example,

$$ a = \{1, 2, 0, 0, 7\} $$ $$b = \{1, 3, 4, 0, 0\} $$ Then Hamming distance$(a,b) = 3$.

\vspace{.1in}

In this application, we have two sets of vectors that we're interested in comparing. I consider two ways to calculate the Hamming distance between two sets of vectors. Let the training data for subject 1 be represented as follows, where we have $k$ predictors and $j$ rows of data. $$X_{1, train}= \begin{bmatrix} x_{11}& x_{12} &\dots& x_{1 k} \\
  \vdots & \vdots & \vdots & \vdots \\
  x_{j 1} & x_{j2} & \dots & x_{jk} \end{bmatrix}$$

Then we can take the mean of each column and round to the nearest integer to get a "summary" vector for the training data, and do the same for the testing data. $$\overline{X}_{1, train} =  \Big[\big\lfloor\frac{1}{j}\sum_{i=1}^jx_{i1}\big\rfloor, \big\lfloor\frac{1}{j}\sum_{i=1}^jx_{i2}\big\rfloor, \dots,  \big\lfloor\frac{1}{j}\sum_{i=1}^j x_{ik}\big\rfloor\Big] $$

Then we would calculate Hamming($\overline{X}_{1, train},\overline{X}_{1, test}$, Hamming($\overline{X}_{1, train},\overline{X}_{2, test}$, and so on for all pairwise combinations of training subjects and testing subjects. In total, there are $32*32 = 1024$ pairs to test.

\vspace{.1in}

We could instead take the median of each column to get the "summary" vector and calculate the Hamming distances as above with each pairwise combination.

$$\widetilde{X}_{1, train} = \Big[med(x_{11}, x_{21}, \dots, x_{j1}), med(x_{12}, x_{22}, \dots, x_{j2}),\dots, med(x_{1k}, x_{2k}, \dots, x_{jk})\Big] $$

### Mean Method

```{r}
train <- rep(seq(1,32,1), each=32)
test <- rep(seq(1,32,1), 32)
calculate_distance_mean <- function(subject_train, subject_test, data_train, data_test,binary=FALSE){
  n <- ncol(data_train %>% dplyr::select(-ID))
    train_vector <- data_train %>% filter(ID==subject_train) %>% dplyr::select(-ID) %>% summarize(across(all_of(1:n), ~round(mean(.x), 0)))
    test_vector <- data_test %>% filter(ID==subject_test) %>% dplyr::select(-ID) %>% summarize(across(all_of(1:n), ~round(mean(.x), 0)))
  sum(train_vector!=test_vector)
}

distances_mean <- map2_dbl(.x = train, .y = test,
                          data_train = data_train, data_test=data_test, 
                          .f = calculate_distance_mean)

distances_mat <- matrix(distances_mean, nrow=32)
distances_df <- distances_mat %>% data.frame() 
colnames(distances_df) <- seq(1,32,1)
distances_df <- distances_df %>% mutate(test_subj = seq(1,32,1))

# i,jth entry is ith subject testing, jth subject training 
```

```{r}
long <- distances_df %>% pivot_longer(cols = -test_subj) %>% mutate(name = as.numeric(name)) 
long$scale <- scale(long$value, center=F)
ggplot(long, aes(x=test_subj, y =name, fill = scale))+
  geom_tile()+scale_fill_viridis_c(name = "Normalized Distance", option="magma")+scale_x_continuous(breaks=seq(1,32,1))+
  scale_y_continuous(breaks=seq(1,32,1))+labs(x = "Test Data Subject", y = "Training Data Subject",
                                              title = "Distances between Training and Testing Data for Each Subject", subtitle = "Mean Method")+
  geom_tile(data = long %>% filter(test_subj==name), fill = NA, color = "white", linewidth=.5)
# keep playing w this 

```

### Median Method

```{r}
calculate_distance <- function(subject_train, subject_test, data_train, data_test,binary=FALSE){
  n <- ncol(data_train %>% dplyr::select(-ID))
  if(base::isFALSE(binary)){
    train_vector <- data_train %>% filter(ID==subject_train) %>% dplyr::select(-ID) %>% summarize(across(all_of(1:n), median))
    test_vector <- data_test %>% filter(ID==subject_test) %>% dplyr::select(-ID) %>% summarize(across(all_of(1:n), median))
  }
  else{
    train_vector <- data_train %>% filter(ID==subject_train) %>% dplyr::select(-ID) %>% summarize(across(all_of(1:n), median)) %>% mutate(across(1:n, ~ifelse(.x>0, 1, 0)))
    test_vector <- data_test %>% filter(ID==subject_test) %>% dplyr::select(-ID) %>% summarize(across(all_of(1:n), median)) %>% mutate(across(1:n, ~ifelse(.x>0, 1, 0)))
  }
  sum(train_vector!=test_vector)
}

distances <- map2_dbl(.x = train, .y = test,
                    data_train = data_train, data_test=data_test, binary=FALSE,
                    .f = calculate_distance)

```

```{r}
distances_mat <- matrix(distances, nrow=32)
# i,jth entry is ith subject testing, jth subject training 

distances_df <- distances_mat %>% data.frame() 
colnames(distances_df) <- seq(1,32,1)
distances_df <- distances_df %>% mutate(test_subj = seq(1,32,1))

# max distance = 864, min distance = 0 

long <- distances_df %>% pivot_longer(cols = -test_subj) %>% mutate(name = as.numeric(name)) 
long$scale <- scale(long$value, center=F)

ggplot(long, aes(x=test_subj, y =name, fill = scale))+
  geom_tile()+scale_fill_viridis_c(name = "Normalized Distance", option="magma")+scale_x_continuous(breaks=seq(1,32,1))+
  scale_y_continuous(breaks=seq(1,32,1))+labs(x = "Test Data Subject", y = "Training Data Subject", 
                                              title = "Distances between Training and Testing Data for Each Subject", subtitle = "Median Method")+
  geom_tile(data = long %>% filter(test_subj==name), fill = NA, color = "white", linewidth=.5)

```

### A Different Approach to Distances

We can think of our data as a 3D image, like these:

```{r}

train_samp <- df_all %>% filter(ID2==2) %>% mutate(maxJ = max(J)) %>% filter(J <= 0.75*maxJ) %>% dplyr::select(signal_lw, time_s_2) %>% mutate(
      lag_signal = lag(signal_lw, n = 15)  # for each second, calculate signal and lagged signal 
    ) %>% filter(!is.na(lag_signal)) %>% mutate(type = "train")
test_samp <- df_all %>% filter(ID2==2) %>% mutate(maxJ = max(J)) %>% filter(J > 0.75*maxJ) %>% dplyr::select(signal_lw, time_s_2) %>% mutate(
      lag_signal = lag(signal_lw, n = 15)  # for each second, calculate signal and lagged signal 
    ) %>% filter(!is.na(lag_signal)) %>% mutate(type = "test")

#bind_rows(train_samp, test_samp) %>% ggplot(aes(x = lag_signal, y = signal_lw))+geom_density_2d_filled(contour_var="ndensity")+
 # facet_wrap(.~type)
bind_rows(train_samp, test_samp) %>% ggplot(aes(x = lag_signal, y = signal_lw))+stat_density_2d(geom = "raster", aes(fill = after_stat(ndensity)), contour=F)+scale_fill_viridis_c(name = "Density")+
  facet_wrap(.~type)+labs(x = "Lag Signal", y = "Signal", title = "Training and Testing Data for Subject 2", subtitle = "Lag 15sec")

test_samp <- df_all %>% filter(ID2==3) %>% mutate(maxJ = max(J)) %>% filter(J > 0.75*maxJ) %>% dplyr::select(signal_lw, time_s_2) %>% mutate(
      lag_signal = lag(signal_lw, n = 15)  # for each second, calculate signal and lagged signal 
    ) %>% filter(!is.na(lag_signal)) %>% mutate(type = "test")

bind_rows(train_samp, test_samp) %>% ggplot(aes(x = lag_signal, y = signal_lw))+stat_density_2d(geom = "raster", aes(fill = after_stat(ndensity)), contour=F)+scale_fill_viridis_c(name = "Density")+
  facet_wrap(.~type)+labs(x = "Lag Signal", y = "Signal", title = "Training Data for Subject 2, Testing Data for Subject 3",
                          subtitle = "Lag 15sec")
```

We could represent these images as matrices, where entry $x_{ij} = \text{density or number of points}$ where signal is in interval $i$ and lag signal is in interval $j$. Using intervals of 0.25 $m/s^2$, this would result in a 12x12 matrix for each a given lag. For example, the matrix for subject 2, training data, lag 0.15 second is below:

```{r}
## below is matrix representation of the image 
 testing <- data_train %>% filter(ID==2) %>% dplyr::select(1:146) %>% dplyr::select(-c(ID)) %>% summarize(across(1:144, sum)) %>% pivot_longer(cols = 1:144) %>% rowwise() %>% mutate(x = str_split(name, " ")[[1]][1], y = str_split(name, " ")[[1]][2]) %>% dplyr::select(-name) %>% pivot_wider(names_from = y, values_from = value, id_cols = x) %>% dplyr::select(-x)
testing 
```

We scale by the number of points to make the result easy to compare for different lags:

```{r,eval=F}
scaled <- testing/sum(rowSums(as.matrix(testing))); scaled
```

For each subject, we can calculate the matrix of the image for all of the lags, in the training and the testing data, then we can compute the squared Frobenius distance between all combinations of training and testing data.

```{r}

get_images <- function(subject, lags, data){
  get_image_onelag <- function(lag, subject, data){
    image <- data %>% filter(ID==subject) %>% dplyr::select(contains(lag)) %>% summarize(across(1:144, sum)) %>% pivot_longer(cols = 1:144) %>% rowwise() %>% mutate(x = str_split(name, " ")[[1]][1], y = str_split(name, " ")[[1]][2]) %>% dplyr::select(-name) %>% pivot_wider(names_from = y, values_from = value, id_cols = x) %>% dplyr::select(-x)
    return(image/sum(rowSums(as.matrix(image))))
    }
  return(map_dfr(.x = lags, .f = get_image_onelag, subject=subject, data=data))
  
}
lags <- c(" 15", " 30", " 45", " 60", " 75", " 90")
all_images_training <- map(.x = subjects, lags = lags, data = data_train, .f = get_images)
all_images_testing <- map(.x = subjects, lags = lags, data = data_test, .f = get_images)
train <- rep(seq(1,32,1), each=32)
test <- rep(seq(1,32,1), 32)

calculate_frob_distance <- function(subject_train, subject_test, list_train, list_test){
  train_mat <- as.matrix(list_train[[subject_train]])
  test_mat <- as.matrix(list_test[[subject_test]])
  SMFilter::FDist2(train_mat, test_mat)
}

distances_frob <- map2_dbl(.x = train, .y = test,
                          list_train = all_images_training, list_test = all_images_testing,
                          .f = calculate_frob_distance)

distances_fmat <- matrix(distances_frob, nrow=32)
distances_df <- distances_fmat %>% data.frame() 
colnames(distances_df) <- seq(1,32,1)
distances_df <- distances_df %>% mutate(test_subj = seq(1,32,1))

long <- distances_df %>% pivot_longer(cols = -test_subj) %>% mutate(name = as.numeric(name)) 

ggplot(long, aes(x=test_subj, y =name, fill = value))+
  geom_tile()+scale_fill_viridis_c(name = "Frobenius Distance", option="magma")+scale_x_continuous(breaks=seq(1,32,1))+
  scale_y_continuous(breaks=seq(1,32,1))+labs(x = "Test Data Subject", y = "Training Data Subject",
                                              title = "Distances between Training and Testing Data for Each Subject", subtitle = "")+
  geom_tile(data = long %>% filter(test_subj==name), fill = NA, color = "white", linewidth=.5)
# keep playing w this 

```

### Comparing "Distances" and Predictive Performance

We can look at which subjects are the best predicted:

```{r}
differences <- all_predictions %>%  group_by(true_subject, model) %>% summarize(mean_pred = mean(pred)) %>%  
    group_by(true_subject) %>% summarize(
      maxprob = max(mean_pred),
      prediction = model[mean_pred==maxprob], # which subject has highest predicted probability 
      probsubj = mean_pred[true_subject==model],
      second_largest_prob = mean_pred[rank(-mean_pred, ties.method="average")==2],
      diff = maxprob - second_largest_prob,
      avgdiff = maxprob - mean(mean_pred[mean_pred!=maxprob])
      # which probability comes from the "true" model 
    ) %>% mutate(
      correct = ifelse(as.numeric(prediction)==true_subject, 1, 0)
    ) %>% dplyr::select(true_subject, diff, avgdiff, correct)

prob_matrix <- all_predictions %>%  group_by(true_subject, model) %>% summarize(mean_pred = mean(pred)) %>% group_by(true_subject) %>%
  mutate(
    maxprob = max(mean_pred),
    prediction = model[mean_pred==maxprob],
    correct = ifelse(prediction==true_subject, 1, 0)
  )
ggplot(prob_matrix, aes(x = true_subject, y = model, fill = mean_pred))+geom_tile()+
  scale_fill_viridis_c(name = "Predicted Probability", option="inferno")+scale_x_continuous(breaks=seq(1,32,1))+
  scale_y_continuous(breaks=seq(1,32,1))+labs(x = "True Subject", y = "Test Subject", 
                                              title = "Predicted Probabilities", subtitle = "")+
  geom_tile(data = prob_matrix %>% filter(mean_pred==maxprob), fill = NA, color = "white", linewidth=.5)

```

For each subject, we can also look at the difference between the correct probability and the second highest probability:

```{r}
differences <- all_predictions %>%  group_by(true_subject, model) %>% summarize(mean_pred = mean(pred)) %>%  
    group_by(true_subject) %>% summarize(
      maxprob = max(mean_pred),
      prediction = model[mean_pred==maxprob], # which subject has highest predicted probability 
      probsubj = mean_pred[true_subject==model],
      second_largest_prob = mean_pred[rank(-mean_pred, ties.method="average")==2],
      diff = maxprob - second_largest_prob,
      avgdiff = maxprob - mean(mean_pred[mean_pred!=maxprob])
      # which probability comes from the "true" model 
    ) %>% mutate(
      correct = ifelse(as.numeric(prediction)==true_subject, 1, 0)
    ) %>% dplyr::select(true_subject, diff, avgdiff, correct)
tab1 <- differences %>% dplyr::select(true_subject, diff, avgdiff) %>% arrange(desc(diff)) %>% slice(1:16) %>% gt() %>%
  tab_header(title = "Prediction Summary") %>% cols_label(true_subject = "Subject",
                                                          diff = "Dist Next Highest Prob", avgdiff = "Dist Mean Other Probs") %>% fmt_number(columns  = 2:3, decimals=2)

tab2 <- differences %>% dplyr::select(true_subject, diff, avgdiff) %>% arrange(desc(diff)) %>% slice(17:32) %>% gt() %>%
  tab_header(title = "Prediction Summary") %>% cols_label(true_subject = "Subject",
                                                          diff = "Dist Next Highest Prob", avgdiff = "Dist Mean Other Probs") %>% fmt_number(columns  = 2:3, decimals=2)
gtExtras::gt_two_column_layout(list(tab1, tab2), vwidth=250)
```

```{r, eval=F}
get_grids <- function(subject, n_predictors, train, test, threshold){
  # first filter to grids above threshold 
  grids <- train %>% filter(ID==subject) %>% pivot_longer(cols = -ID) %>% 
    mutate(lag = sub("^.+] ", "", name)) %>% group_by(name, lag) %>% 
    summarize(value = sum(value)) %>% group_by(lag) %>% mutate(
      group_sum = sum(value)
    ) %>% ungroup() %>% mutate(
      pct = value/group_sum
    ) %>% filter(pct >= threshold) %>% dplyr::select(name)  %>% unlist() %>% unname()
  # make outcome binary 
  train$class <- ifelse(train$ID==subject, 1, 0)
  # with the training data, fit a logistic regression with each predictor individually and get p-value 
  # select __ lowest p values based on n_predictors input 
  imp_grids <- train %>% dplyr::select(all_of(grids)) %>%
    map(~glm(train$class ~.x, data = train, family = binomial(link="logit"))) %>%
    map(summary.glm) %>% map(c("coefficients")) %>% map_dbl(8) %>% as.data.frame() %>% rename("pval" = ".") %>% slice_min(pval, n=n_predictors) %>% rownames()
  grid_df <- train %>% dplyr::select(c(ID, all_of(imp_grids))) %>% summarize(across(starts_with("("), mean)) %>% mutate(
    ID = subject
  ) %>% pivot_longer(cols = -ID) %>% rename("grid" = name, "avg" = value)
return(grid_df)
}

grids <- map_dfr(.x = subjects, .f = get_grids, n_predictors = 10, train= data_train, test = data_test, threshold = 0.001)

selected_grids <- grids %>% group_by(grid) %>% count() %>% arrange(desc(n)) %>% filter(n>=4) %>% dplyr::select(grid) %>% unlist() %>% unname()

data_train %>% dplyr::select(c(ID, all_of(selected_grids))) %>% group_by(ID) %>% summarize(across(starts_with("("), sum)) %>% pivot_longer(cols=-ID) %>% rename(
  "grid" = name, 
  "count"=value
) %>% ggplot(aes(x = as.factor(grid), y = ID, fill = count))+geom_tile()+scale_fill_viridis()+
      theme(axis.text.x = element_text(angle = 90))+scale_y_continuous(breaks=seq(1,32,1))
```

```{r, eval=F}


train_sub1 <- data_train %>% filter(ID==1) %>% dplyr::select(-ID) %>% mutate(across(1:864, ~ifelse(.x>0, 1, 0))) %>% as.matrix()
test_sub1 <- data_test %>% filter(ID==1) %>% dplyr::select(-ID) %>% mutate(across(1:864, ~ifelse(.x>0, 1, 0))) %>% as.matrix()
test_sub2 <- data_test %>% dplyr::filter(ID==2) %>% dplyr::select(-ID) %>% mutate(across(1:864, ~ifelse(.x>0, 1, 0))) %>% as.matrix()

mean(gsignal::wconv(type = "2d", test_sub1, train_sub1, shape = "same"))
mean(gsignal::wconv(type = "2d", test_sub2, train_sub1, shape = "same"))

test_sub2 <- data_test %>% dplyr::filter(ID==2) %>% dplyr::select(-ID) %>% as.matrix()
test_sub4 <- data_test %>% dplyr::filter(ID==3) %>% dplyr::select(-ID) %>% as.matrix()
result <- gsignal::wconv(type = "2d", train_sub1, test_sub1, shape = "same")
result <- gsignal::wconv(type = "2d", test_sub1, train_sub1, shape = "same")
result2 <- gsignal::wconv(type = "2d", test_sub2, train_sub1, shape = "same")

resultC <- gsignal::wconv(type = "column", train_sub1, test_sub1)
result2 <-gsignal::wconv(type = "2d", train_sub1, test_sub2, shape = "same")
result4 <-gsignal::wconv(type = "2d", train_sub1, test_sub4, shape = "same")

# OpenImageR::convlution(train_sub1, test_sub1, mode = "same")
norm(result, type = "F"); norm(result2, type="F"); norm(result4, type="F")
mean(result); mean(result2); mean(result4)


```

\vspace{.3 in}

### Next steps

```{=tex}
\begin{itemize}
\item Image similarity? 
\item Clustering based on images? 
\end{itemize}
```
```{r, eval=F}
# function to save images 
location <- "lw"
training_pct = .75
lag <- 15

plot_dens <- function(subject, training_pct, location, lag, data){ # inputs are subject, pct of training, location, lag (ms), data
  sig <- paste("signal_", location, sep="")
  newname <- c(signal = sig, t = "t")
  g <- data %>% filter(ID2==subject) %>% mutate(maxJ = max(J)) %>% filter(J <= training_pct*maxJ) %>% dplyr::select(sig, t) %>% rename(all_of(newname)) %>% mutate(
      lag_signal = lag(signal, n = lag)) %>% filter(!is.na(lag_signal)) %>% ggplot(aes(x = lag_signal, y = signal))+stat_density_2d(geom = "raster", aes(fill = after_stat(ndensity)), contour=F)+scale_fill_viridis_c(name = "Density") + theme_void() + scale_x_continuous(limits=c(0,4))+scale_y_continuous(limits=c(0,4))+theme(legend.position="none")+labs(x="", y = "")
  name = paste("subj_", subject, "_train_lag_", lag, ".jpeg", sep="")
  ggsave(filename = name, path = "~/Documents/walking_fingerprint_IU/images")
 g<-  data %>% filter(ID2==subject) %>% mutate(maxJ = max(J)) %>% filter(J > training_pct*maxJ) %>% dplyr::select(sig, t) %>% rename(all_of(newname)) %>% mutate(
      lag_signal = lag(signal, n = lag)) %>% filter(!is.na(lag_signal)) %>% ggplot(aes(x = lag_signal, y = signal))+stat_density_2d(geom = "raster", aes(fill = after_stat(ndensity)), contour=F)+scale_fill_viridis_c(name = "Density") + theme_void() + scale_x_continuous(limits=c(0,4))+scale_y_continuous(limits=c(0,4))+ theme(legend.position="none")+labs(x="", y = "")
  name = paste("subj_", subject, "_test_lag_", lag, ".jpeg", sep="")
 ggsave(filename = name, path = "~/Documents/walking_fingerprint_IU/images")
}

map(.x = subjects, .f = plot_dens, data = df_all, location="lw", lag=15, training_pct = .75)

library(jpeg)

imgs <- list.files("/Users/lilykoffman/Documents/walking_fingerprint_IU/images", full.names = T)
img1 <- readJPEG(imgs[1])
img3 <- readJPEG(imgs[3])
dim(img1)[1:2]
img2 <- readJPEG(imgs[2]); dim(img2)[1:2]
RNiftyReg::similarity(img1, img2)
RNiftyReg::similarity(img3, img2)
```
